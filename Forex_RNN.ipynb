{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deniseiras/Forex_RNN/blob/main/Forex_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h-LhRbMzE7A"
      },
      "source": [
        "---\n",
        "# Rede RNN (LSTM) - Moeda\n",
        "\n",
        "Denis M. A. Eiras\n",
        "Atualizado: 14/09/2023\n",
        "\n",
        "O notebook objetiva fazer a predição das moedas EURO e DÓLAR dos últimos 10 dias e comparar com os dados observados. Ainda, fazer a predição futura dos próximos 10 dias.\n",
        "\n",
        "O dataset é disponibilizado através do uso da API yahoo finance.\n",
        "\n",
        "Tempo aproximado de execução de xxx minutos\n",
        "\n",
        "# TODO - O dataframe ... serve só para a predição diária"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j04WaiXBz_kj"
      },
      "source": [
        "## Inicialização das bibliotecas e parâmetros gerais\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq5EzZ2Rr_yN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import Model, Sequential, model_from_json\n",
        "from keras.layers import LSTM, Input, Dropout, Dense, RepeatVector, TimeDistributed, Conv1D, Flatten, MaxPooling1D, \\\n",
        "  BatchNormalization, Activation\n",
        "from keras.regularizers import l2\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "#import glob\n",
        "!pip install yfinance\n",
        "import yfinance as yf\n",
        "from IPython import display\n",
        "import sys\n",
        "import warnings\n",
        "from time import time\n",
        "from datetime import datetime, timedelta\n",
        "from random import uniform\n",
        "import json\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"\")\n",
        "print(\"Python version:\")\n",
        "print (sys.version)\n",
        "print(\"\\nTensorflow version:\")\n",
        "print(tf.__version__)\n",
        "print('')\n",
        "\n",
        "starts_at = datetime.now()\n",
        "print(f'Starting at {starts_at}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Informações sobre o dólar USD"
      ],
      "metadata": {
        "id": "Ug0bgvwag7cR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8XgMK4DvHJP"
      },
      "outputs": [],
      "source": [
        "eurobrl = \"EURBRL=X\"\n",
        "usdbrl = \"USDBRL=X\"\n",
        "btcusd = \"BTC-USD\"\n",
        "msft = yf.Ticker(usdbrl)\n",
        "\n",
        "# get stock info\n",
        "print(msft.info)\n",
        "\n",
        "# get historical market data\n",
        "hist = msft.history(period=\"max\")\n",
        "\n",
        "# show actions (dividends, splits)\n",
        "# print(msft.actions)\n",
        "\n",
        "# show dividends\n",
        "print(msft.dividends)\n",
        "\n",
        "# show splits\n",
        "print(msft.splits)\n",
        "\n",
        "# show major holders\n",
        "print(msft.major_holders)\n",
        "\n",
        "# show institutional holders\n",
        "print(msft.institutional_holders)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtMAqv050ETC"
      },
      "source": [
        "## Leitura e tratamento do Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjEaZpFxPGnI"
      },
      "source": [
        "Requisição de dados para API *Yahoo Finance* e visualização de informações básicas.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OaQnkhmoPJj"
      },
      "outputs": [],
      "source": [
        "arr_acoes = [eurobrl, usdbrl]\n",
        "\n",
        "data = yf.download(arr_acoes)\n",
        "\n",
        "df = data[\"Close\"]\n",
        "df.dropna(axis=0, inplace=True)\n",
        "print(\"Data Inicial: \", df.index.min())\n",
        "print(\"Data Final  : \", df.index.max())\n",
        "print(\"Tamanho     : \", len(df))\n",
        "print('')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 10), nrows=1, ncols=1)\n",
        "sns.lineplot(data=df, palette='inferno', ax=ax,hue_order=arr_acoes)\n",
        "ax.set_xlabel('Anos')\n",
        "ax.set_title('Preço de Encerramento')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87RnmZNrLfl9"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwFNrNICuUFj"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNSefaeILhYo"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWaTlBEGQysO"
      },
      "source": [
        "Seleção e visualização do dataset de Treino e Teste.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZqO6wSkGb3u"
      },
      "outputs": [],
      "source": [
        "today = datetime.today()\n",
        "\n",
        "# prev últimos dias úteis\n",
        "dias_teste = 20\n",
        "\n",
        "data_corte_treino = today - timedelta(days=1)\n",
        "while True:\n",
        "  data_treino = df.loc[(df.index <= data_corte_treino)]\n",
        "  data_teste = df.loc[(df.index > data_corte_treino)]\n",
        "  if len(data_teste) == dias_teste:\n",
        "    break\n",
        "  data_corte_treino = data_corte_treino - timedelta(days=1)\n",
        "\n",
        "\n",
        "print(\"Data Inicial Treino: \", data_treino.index.min())\n",
        "print(\"Data Final   Treino: \", data_treino.index.max())\n",
        "print(\"Tamanho      Treino: \", len(data_treino))\n",
        "print('')\n",
        "\n",
        "print(\"Data Inicial Teste: \", data_teste.index.min())\n",
        "print(\"Data Final   Teste: \", data_teste.index.max())\n",
        "print(\"Tamanho      Teste: \", len(data_teste))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8), nrows=1, ncols=1)\n",
        "sns.lineplot(data=data_treino, palette=\"inferno\", ax=ax, hue_order=arr_acoes)\n",
        "ax.set_xlabel('Anos')\n",
        "ax.set_title('Dataset de Treino')\n",
        "# ax.set_ylim([0, 80])\n",
        "plt.show()\n",
        "print('')\n",
        "\n",
        "\n",
        "# variáveis não usadas para treino, apenas visualização\n",
        "tmp_data_corte_treino_30dias = today - timedelta(days=30)\n",
        "tmp_data_treino_30dias = df.loc[(df.index >= tmp_data_corte_treino_30dias)]\n",
        "fig, ax = plt.subplots(figsize=(12, 8), nrows=1, ncols=1)\n",
        "sns.lineplot(data=tmp_data_treino_30dias, palette=\"inferno\", ax=ax, hue_order=arr_acoes)\n",
        "ax.set_xlabel('Dias')\n",
        "ax.set_title('Dataset de Treino - último mês')\n",
        "# ax.set_ylim([0, 80])\n",
        "plt.show()\n",
        "print('')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8), nrows=1, ncols=1)\n",
        "sns.lineplot(data=data_teste, palette=\"inferno\", ax=ax, hue_order=arr_acoes)\n",
        "ax.set_xlabel('Dias')\n",
        "ax.set_title(f'Dataset de Teste - últimos {dias_teste} dias')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOiqTYGADDTP"
      },
      "source": [
        "## Pré Processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-addP9faRFq6"
      },
      "source": [
        "Função utilizada na transformação da base de dados em 3 dimensões, contendo:\n",
        "\n",
        "1.   Uma sequência ou mais de exemplos (Batch Size);\n",
        "2.   Um ponto ou mais de observações em série temporal (Time Steps);\n",
        "3.   Numero de váriaveis contidas em cada Time Step (Features).\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTx6W1LODK4q"
      },
      "outputs": [],
      "source": [
        "def preprocess(dataset, stock, TimeSteps, TesteLen):\n",
        "\n",
        "    # StandardScaler\n",
        "    n = len(dataset[[stock]])\n",
        "    scaler = StandardScaler()\n",
        "    scaler = scaler.fit(dataset[[stock]])\n",
        "\n",
        "    df_scal = scaler.transform(dataset[[stock]])\n",
        "    df_scal = df_scal.reshape(n, 1)\n",
        "\n",
        "    X = df_scal\n",
        "    X_samples = list()\n",
        "    y_samples = list()\n",
        "\n",
        "    NumerOfRows = len(X)\n",
        "    for i in range(int(TimeSteps) , NumerOfRows , 1):\n",
        "        x_sample = X[i-TimeSteps:i]\n",
        "        y_sample = X[i]\n",
        "        X_samples.append(x_sample)\n",
        "        y_samples.append(y_sample)\n",
        "\n",
        "    X_data=np.array(X_samples)\n",
        "    X_data=X_data.reshape(X_data.shape[0],X_data.shape[1], 1)\n",
        "\n",
        "    y_data=np.array(y_samples)\n",
        "    y_data=y_data.reshape(y_data.shape[0], 1)\n",
        "\n",
        "    # Separando dataset entre treino e teste\n",
        "    if TesteLen > 0:\n",
        "      X_train=X_data[:-TesteLen]\n",
        "      X_test=X_data[-TesteLen:]\n",
        "      y_train=y_data[:-TesteLen]\n",
        "      y_test=y_data[-TesteLen:]\n",
        "    else:\n",
        "      X_train=X_data\n",
        "      X_test=[]\n",
        "      y_train=y_data\n",
        "      y_test=[]\n",
        "    return scaler, X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def preprocess_CNN1D(dataset, stock, TesteLen):\n",
        "    # StandardScaler\n",
        "    n = len(dataset[[stock]])\n",
        "    scaler = StandardScaler()\n",
        "    scaler = scaler.fit(dataset[[stock]])\n",
        "\n",
        "    df_scal = scaler.transform(dataset[[stock]])\n",
        "    df_scal = df_scal.reshape(n, 1)\n",
        "\n",
        "    X = np.array(df_scal)\n",
        "    # Separando dataset entre treino e teste\n",
        "    if TesteLen > 0:\n",
        "      X_train = X[:-TesteLen]\n",
        "      X_test = X[-TesteLen:]\n",
        "    else:\n",
        "      X_train = X_train\n",
        "      X_test = []\n",
        "\n",
        "    y_train, y_test = X_train, X_test\n",
        "    X_train = X_train.reshape(X_train.shape[0], 1, 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], 1, 1)\n",
        "\n",
        "    return scaler, X_train, X_test, y_train, y_test\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTrYcBO3UGCh"
      },
      "source": [
        "Exemplo de transformação do dataset, utilizando 10 observações por batch, separando os ultimos len(data_teste) registros para avaliação dos melhores modelos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpIv6oEdaSfM"
      },
      "outputs": [],
      "source": [
        "timesteps = 10\n",
        "print(\"LSTM data\\n\\n\")\n",
        "scaler, X_train, X_test, y_train, y_test = preprocess(df, eurobrl, timesteps, len(data_teste))\n",
        "for inp, out in zip(X_train[0:3], y_train[0:3]):\n",
        "  print(inp,'--', out)\n",
        "\n",
        "print(\"CNN1D data\\n\\n\")\n",
        "scaler, X_train, X_test, y_train, y_test = preprocess_CNN1D(df, eurobrl, len(data_teste))\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "for inp, out in zip(X_train[0:3], y_train[0:3]):\n",
        "  print(inp,'--', out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk5npYTADMgw"
      },
      "source": [
        "## Construção dos modelos RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTNsvDAGSqHw"
      },
      "source": [
        "Para desenvolvimento do projeto foram testadas dois modelos de RNN, ambas submetidas ao mesmo dataset e a variação em paramêtros.\n",
        "\n",
        "Modelo v1:\n",
        "- 1 camada LSTM com ativação relu\n",
        "- 1 camada Densa\n",
        "- 1 camada de ativação linear\n",
        "\n",
        "Modelo v2:\n",
        "- 1 camada LSTM com ativação relu\n",
        "- 1 camada Dropout 10%\n",
        "- 1 camada LSTM com ativação relu e 1/4 do número de neurônios como input\n",
        "- 1 camada Dropout 10%\n",
        "- 1 camada Densa\n",
        "- 1 camada de ativação linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUKhkKzEsIz5"
      },
      "outputs": [],
      "source": [
        "def train_model(mod, optimizer, X, y, batch_size, patience, epochs):\n",
        "\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_mean_absolute_error', patience=patience, mode='min',\n",
        "                                              restore_best_weights=True)\n",
        "\n",
        "  # only Adam implemented\n",
        "  if optimizer == \"Adam\":\n",
        "    adam = tf.keras.optimizers.Adam()\n",
        "    mod.compile(loss=tf.losses.MeanSquaredError(),optimizer=adam, metrics=[tf.metrics.MeanAbsoluteError(), tf.metrics.MeanSquaredError()])\n",
        "\n",
        "  display.display(mod.summary())\n",
        "  hist = mod.fit(X, y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[callback], verbose=0,\n",
        "                  shuffle=False)\n",
        "\n",
        "  loss_metric_train = mod.evaluate(X, y, verbose=0, batch_size=batch_size, use_multiprocessing=True)\n",
        "  display.display( 'TRAINING set. Loss = {} , MAE = {}'.format(loss_metric_train[0], loss_metric_train[1]))\n",
        "\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.plot(hist.history['loss'], label=\"Loss Treino\")\n",
        "  plt.plot(hist.history['val_loss'], label=\"Loss Validação\")\n",
        "  plt.plot(hist.history['mean_absolute_error'], label='MAE Validação')\n",
        "  plt.ylim([0.0, 0.2])\n",
        "  plt.xlim([0.0, epochs])\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  return mod, hist\n",
        "\n",
        "\n",
        "def model_type(mod_type, unit, optimizer, timesteps, X, y, epochs=60, batch_size=32, patience=5):\n",
        "  func = globals()[mod_type]\n",
        "  return func(unit, optimizer, timesteps, X, y, epochs, batch_size, patience)\n",
        "\n",
        "\n",
        "def LSTM_v1(unit, optimizer, timesteps, X, y, epochs, batch_size, patience):\n",
        "\n",
        "    mod = Sequential()\n",
        "    mod.add(LSTM(int(unit), activation='relu', input_shape=(timesteps, 1), return_sequences=False))\n",
        "    mod.add(Dense(1, activation='linear'))\n",
        "\n",
        "    mod, hist = train_model(mod, optimizer, X, y, batch_size, patience, epochs)\n",
        "\n",
        "    return mod, hist\n",
        "\n",
        "\n",
        "def LSTM_v2(unit, optimizer, timesteps, X, y, epochs, batch_size, patience, l2_reg=0.001):\n",
        "\n",
        "    mod = Sequential()\n",
        "    mod.add(LSTM(int(unit), activation='relu', input_shape=(timesteps, 1), kernel_regularizer=l2(l2_reg),\n",
        "                 return_sequences=False))\n",
        "    mod.add(Dense(1, activation='linear'))\n",
        "\n",
        "    mod, hist = train_model(mod, optimizer, X, y, batch_size, patience, epochs)\n",
        "\n",
        "    return mod, hist\n",
        "\n",
        "\n",
        "def LSTM_v3(unit, optimizer, timesteps, X, y, epochs, batch_size, patience, dropout_rate=0.3, l2_reg=0.001):\n",
        "\n",
        "    mod = Sequential()\n",
        "    mod.add(LSTM(int(unit), activation='relu', input_shape=(timesteps, 1), kernel_regularizer=l2(l2_reg),\n",
        "                 return_sequences=False))\n",
        "    mod.add(Dropout(dropout_rate))\n",
        "    mod.add(Dense(1, activation='linear'))\n",
        "\n",
        "    mod, hist = train_model(mod, optimizer, X, y, batch_size, patience, epochs)\n",
        "\n",
        "    return mod, hist\n",
        "\n",
        "\n",
        "def LSTM_v4(unit, optimizer, timesteps, X, y, epochs, batch_size, patience, dropout_rate=0.3):\n",
        "\n",
        "    mod = Sequential()\n",
        "    mod.add(LSTM(int(unit), activation='relu', input_shape=(timesteps, 1), return_sequences=True))\n",
        "    mod.add(Dropout(0.1))\n",
        "    mod.add(LSTM(int(unit/4), activation='relu', return_sequences=False ))\n",
        "    mod.add(Dropout(0.1))\n",
        "    mod.add(Dense(1, activation='linear'))\n",
        "\n",
        "    mod, hist = train_model(mod, optimizer, X, y, batch_size, patience, epochs)\n",
        "\n",
        "    return mod, hist\n",
        "\n",
        "\n",
        "def CNN_v1(unit, optimizer, timesteps, X, y, epochs, batch_size, patience):\n",
        "\n",
        "  mod = Sequential()\n",
        "  mod.add(Conv1D(filters=unit, kernel_size=2, activation='relu', input_shape=(1,1)))\n",
        "  # mod.add(BatchNormalization())\n",
        "  mod.add(Activation(\"relu\"))\n",
        "  # mod.add(MaxPooling1D(pool_size=3))\n",
        "  mod.add(Flatten())\n",
        "  mod.add(Dense(1, activation='linear'))\n",
        "\n",
        "  mod, hist = train_model(mod, optimizer, X, y, batch_size, patience, epochs)\n",
        "  return mod, hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI5ueuUdnmpk"
      },
      "source": [
        "## Treinamento e Hiperparâmetrização\n",
        "\n",
        "Parâmetros variados\n",
        "\n",
        "1. Modelo: v1 ou v2\n",
        "2. Numéro de Neurônios:  [64, 128]\n",
        "3. Batch size: [16, 32, 64]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F412JH8ysI2T"
      },
      "outputs": [],
      "source": [
        "all_results = pd.DataFrame(columns=[\"model_type\",\"stock\",\"unit\",\"optimizer\", \"batch_size\", \"loss\",\"mse\", \"timesteps\"])\n",
        "\n",
        "arr_acoes_hyperparams = arr_acoes\n",
        "model_hyperparams = ['LSTM_v1', 'LSTM_v2', 'LSTM_v3']\n",
        "unit_hyperparams = [32, 64, 128]\n",
        "batch_hyperparams = [8, 16, 32]\n",
        "arr_timesteps_hyperparams = [10, 20]\n",
        "\n",
        "TotalFeatures = 1\n",
        "epochs = 100\n",
        "times_exec = 3\n",
        "\n",
        "\n",
        "# for testing\n",
        "# arr_acoes_hyperparams = arr_acoes\n",
        "# model_hyperparams = ['LSTM_v1', 'LSTM_v2']\n",
        "# unit_hyperparams = [2, 4]\n",
        "# batch_hyperparams = [8, 16]\n",
        "# arr_timesteps_hyperparams = [5, 10]\n",
        "# TotalFeatures = 1\n",
        "# epochs = 3\n",
        "# times_exec = 2\n",
        "\n",
        "best_models_df = pd.DataFrame(columns=[\"model_type\", \"model\", \"stock\",\"unit\",\"optimizer\", \"batch_size\", \"mse\", \"timesteps\"])\n",
        "\n",
        "for times in range(times_exec):\n",
        "  for stock in arr_acoes_hyperparams:\n",
        "    for timesteps in arr_timesteps_hyperparams:\n",
        "      for model_type_str in model_hyperparams:\n",
        "        if model_type_str[:4] == 'LSTM':\n",
        "          scaler, X_train, X_test, y_train, y_test = preprocess(df, stock, timesteps, len(data_teste))\n",
        "        elif model_type_str[:3] == 'CNN':\n",
        "          scaler, X_train, X_test, y_train, y_test = preprocess_CNN1D(df, stock, len(data_teste))\n",
        "        for optimizer in ['Adam']:\n",
        "          for unit in unit_hyperparams:\n",
        "            for batch_size in batch_hyperparams:\n",
        "              inittime = time()\n",
        "              print(f\"Treinando {stock} com {model_type_str} , units={unit}, batch={batch_size}, timesteps={timesteps}\")\n",
        "              m, h = model_type(model_type_str, unit=unit, optimizer=optimizer, timesteps=timesteps, X=X_train, y=y_train, epochs=epochs, batch_size=batch_size)\n",
        "              endtime = time()\n",
        "              print(f'Tempo de treino: {(endtime-inittime)/60} minutos' )\n",
        "              y_pred = m.predict(X_test)\n",
        "              score = m.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
        "              mod_cfg_res = {'model_type': model_type_str, 'stock':stock,'unit':unit, 'optimizer':optimizer,\n",
        "                                            \"loss\":score[0], \"mae\":score[1], \"mse\":score[2], \"batch_size\":batch_size, \"timesteps\":timesteps}\n",
        "              all_results = all_results.append(mod_cfg_res, ignore_index=True)\n",
        "\n",
        "              mod_cfg_dic = {'model_type': model_type_str, 'stock':stock,'unit':unit, 'optimizer':optimizer,\n",
        "                             \"batch_size\":batch_size, \"timesteps\":timesteps}\n",
        "\n",
        "              # save the model architecture to a JSON string\n",
        "              fprefix = f\"{stock}_{model_type_str}_{unit}_{batch_size}_{timesteps}\"\n",
        "              with open(f\"{fprefix}.json\", \"w\") as json_file:\n",
        "                model_json = m.to_json()\n",
        "                json_file.write(model_json)\n",
        "                # serialize weights to HDF5\n",
        "                m.save_weights(f\"{fprefix}.h5\")\n",
        "\n",
        "              del m, h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dld9qMxzpLu5"
      },
      "source": [
        "## Avaliação na base de testes\n",
        "Desempenho ordenado por moeda e menor mse na base de testes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3KPiNua-wTh"
      },
      "outputs": [],
      "source": [
        "all_results.sort_values(by=['stock','mse'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-PjX7R4GJwM"
      },
      "source": [
        "É considerada a melhor média, em termos de MAE, das três execuções"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKZCZhlNsI4Y"
      },
      "outputs": [],
      "source": [
        "# MElhor médias das execuções\n",
        "print(len(all_results))\n",
        "mean_mse = all_results.groupby(['stock', 'model_type', 'unit', 'batch_size', 'optimizer', 'timesteps'] )['mse'].mean()\n",
        "mean_mse = mean_mse.reset_index()\n",
        "print(len(mean_mse))\n",
        "display.display(mean_mse.sort_values(by=['mse']))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmzXqTNoQZ5Q"
      },
      "source": [
        "Seleção dos melhores modelos de cada arquitetura"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3B_X0oCQawg"
      },
      "outputs": [],
      "source": [
        "\n",
        "best_models_arr = []\n",
        "\n",
        "for model_str in model_hyperparams:\n",
        "  for stock in arr_acoes:\n",
        "    best_model = mean_mse[mean_mse['stock'] == stock].sort_values('mse', ascending=True).head(1)\n",
        "    # best_model = mean_mse[mean_mse['stock'] == stock].sort_values(by=['mse'])\n",
        "    best_model_obj=best_model.iloc[0]\n",
        "\n",
        "    fprefix = f\"{stock}_{best_model_obj['model_type']}_{best_model_obj['unit']}_{best_model_obj['batch_size']}_{best_model_obj['timesteps']}\"\n",
        "    # load json and create model\n",
        "    json_file = open(f'{fprefix}.json', 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    loaded_model = model_from_json(loaded_model_json)\n",
        "    # load weights into new model\n",
        "    loaded_model.load_weights(f\"{fprefix}.h5\")\n",
        "    best_model['model'] = loaded_model\n",
        "\n",
        "    best_models_arr.append(pd.DataFrame(best_model))\n",
        "\n",
        "best_models_df = pd.concat(best_models_arr)\n",
        "del best_models_arr\n",
        "print(\"Melhores modelos\")\n",
        "display.display(best_models_df.sort_values('mse', ascending=True))\n",
        "# display.display(best_models_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aohhFNloAjMr"
      },
      "source": [
        "### Boxplot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TItuOnpTJRxW"
      },
      "source": [
        "Construção dos dados de predição, também usados no Boxplot e Swarmplot\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78YrIMMpAt_C"
      },
      "outputs": [],
      "source": [
        "aval = pd.DataFrame()\n",
        "\n",
        "for index, setup in best_models_df.iterrows():\n",
        "\n",
        "  scaler, X_train, X_test, y_train, y_test = preprocess(df, setup[\"stock\"], setup[\"timesteps\"] , len(data_teste))\n",
        "\n",
        "  mod = setup['model']\n",
        "\n",
        "  y_pred = mod.predict(X_test)\n",
        "  y_pred = scaler.inverse_transform(y_pred)\n",
        "  y_real = scaler.inverse_transform(y_test)\n",
        "\n",
        "  aval[f'{setup[\"stock\"]}'] = y_pred[:,0]\n",
        "  aval[f'{setup[\"stock\"]}_Real'] = y_real[:,0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7QnQywmJbt7"
      },
      "source": [
        "Figura com boxplot e swarmplot para os melhores modelos\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGnbXe5J9a1r"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(20, 7), nrows=1, ncols=2)\n",
        "sns.boxplot(data=aval.filter(regex=usdbrl), dodge=False, palette=\"Blues\", ax=ax[0])\n",
        "sns.swarmplot(data=aval.filter(regex=usdbrl), color=\".25\", alpha=0.8, ax=ax[0])\n",
        "\n",
        "sns.boxplot(data=aval.filter(regex=eurobrl), dodge=False, palette=\"Blues\", ax=ax[1])\n",
        "sns.swarmplot(data=aval.filter(regex=eurobrl), color=\".25\", alpha=0.8, ax=ax[1])\n",
        "\n",
        "ax[1].set_title(\"Performance Boxplot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERk4CMc1_sxt"
      },
      "source": [
        "### Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEL-qCf8OARN"
      },
      "source": [
        "Figura predição por moeda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozqjuptd9ayr"
      },
      "outputs": [],
      "source": [
        "def plot_aval(aval):\n",
        "  fig, ax = plt.subplots(figsize=(30, 12), nrows=1, ncols=2)\n",
        "\n",
        "  sns.lineplot(data=aval.filter(regex=usdbrl), palette=\"inferno\", ax=ax[0], hue_order=[f'{usdbrl}_Real' ,usdbrl])\n",
        "  ax[0].set_xlabel('Dias')\n",
        "  ax[0].set_title(usdbrl)\n",
        "\n",
        "  sns.lineplot(data=aval.filter(regex=eurobrl), palette=\"inferno\", ax=ax[1], hue_order=[f'{eurobrl}_Real', eurobrl])\n",
        "  ax[1].set_xlabel('Dias')\n",
        "  ax[1].set_title(eurobrl)\n",
        "\n",
        "plot_aval(aval)\n",
        "print(aval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ0OxLAmBaxZ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar_iNenB5EcL"
      },
      "source": [
        "## Avaliação no futuro\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0WFvZkk6aCx"
      },
      "outputs": [],
      "source": [
        "aval_fut = pd.DataFrame()\n",
        "\n",
        "sharpness = None\n",
        "test_fut_days = 10\n",
        "for index, setup in best_models_df.iterrows():\n",
        "\n",
        "  scaler, X_train, X_test, y_train, y_test = preprocess(df, setup['stock'], setup[\"timesteps\"], test_fut_days)\n",
        "  inittime = time()\n",
        "  print(f\"Usando {setup['stock']} com {setup['model_type']} , units={setup['unit']}, batch={setup['batch_size']}, \\\n",
        "    timesteps={setup['timesteps']}\")\n",
        "\n",
        "  mod, _ = model_type(setup['model_type'], setup['unit'], setup['optimizer'], setup['timesteps'], X_train, y_train,\n",
        "                      epochs=epochs, batch_size=setup['batch_size'])\n",
        "\n",
        "  m = setup['model']\n",
        "  endtime = time()\n",
        "  print(f'Tempo de treino: {(endtime-inittime)/60} minutos' )\n",
        "\n",
        "  # Initial sequence for prediction\n",
        "  initial_sequence = X_test[0]\n",
        "  # Predict future values iteratively\n",
        "  predicted_values = []\n",
        "\n",
        "  for _ in range(test_fut_days):\n",
        "    # Predict the next step\n",
        "    next_step = m.predict(np.array([initial_sequence]))\n",
        "    if sharpness is not None:\n",
        "      next_step[0, 0] = next_step[0, 0] + next_step[0, 0]*uniform(-sharpness, sharpness)\n",
        "    # Append the predicted value\n",
        "    predicted_values.append(next_step[0, 0])\n",
        "    # Update the initial sequence by removing the oldest value and appending the predicted value\n",
        "    initial_sequence = np.roll(initial_sequence, shift=-1, axis=0)\n",
        "    initial_sequence[-1] = next_step[0, 0]\n",
        "\n",
        "  y_pred = scaler.inverse_transform(np.array(predicted_values).reshape(-1, 1))\n",
        "  y_real = scaler.inverse_transform(y_test)\n",
        "  aval_fut[f'{setup[\"stock\"]}'] = y_pred[:,0]\n",
        "  aval_fut[f'{setup[\"stock\"]}_Real'] = y_real[:,0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMrU4DGN5uPg"
      },
      "source": [
        "### Boxplot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0bLur6f5uPk"
      },
      "source": [
        "Figura com boxplot e swarmplot para os melhores modelos\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMVqX7zZ5uPk"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(20, 7), nrows=1, ncols=2)\n",
        "sns.boxplot(data=aval_fut.filter(regex=usdbrl), dodge=False, palette=\"Blues\", ax=ax[0])\n",
        "sns.swarmplot(data=aval_fut.filter(regex=usdbrl), color=\".25\", alpha=0.8, ax=ax[0])\n",
        "\n",
        "sns.boxplot(data=aval_fut.filter(regex=eurobrl), dodge=False, palette=\"Blues\", ax=ax[1])\n",
        "sns.swarmplot(data=aval_fut.filter(regex=eurobrl), color=\".25\", alpha=0.8, ax=ax[1])\n",
        "\n",
        "ax[1].set_title(\"Performance Boxplot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEvxMH1J5Zqe"
      },
      "source": [
        "### Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsEa8Ahg5Zqf"
      },
      "source": [
        "Figura predição por moeda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXWoUZf05Zqg"
      },
      "outputs": [],
      "source": [
        "plot_aval(aval_fut)\n",
        "print(aval_fut)\n",
        "aval_err = pd.DataFrame(columns=['EURBRL_bias', 'USDBRL_bias'])\n",
        "aval_err['EURBRL_bias'] = 100*abs(aval_fut['EURBRL=X']-aval_fut['EURBRL=X_Real'])/aval_fut['EURBRL=X_Real']\n",
        "aval_err['USDBRL_bias'] = 100*abs(aval_fut['USDBRL=X']-aval_fut['USDBRL=X_Real'])/aval_fut['USDBRL=X_Real']\n",
        "print(aval_err)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ends_at = datetime.now()\n",
        "print(f'End of execution at {ends_at}')\n",
        "print(f'Total time of execution = {ends_at-starts_at}')\n",
        "\n"
      ],
      "metadata": {
        "id": "bn_wzWmqBy-K"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}